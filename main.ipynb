{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re as re\n",
    "import os.path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "\n",
    "# import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "# from tensorflow.keras.callbacks import History,LearningRateScheduler\n",
    "# from tensorflow.keras.layers import Dropout\n",
    "# print('TensorFlow %s, Keras %s, numpy %s, pandas %s'%(tf.__version__,keras.__version__, np.__version__,pd.__version__))\n",
    "__DEBUG__=False\n",
    "\n",
    "train = pd.read_csv('data_concatener.csv', sep=';')\n",
    "test  = pd.read_csv('test_set_values.csv')\n",
    "full_data = [train, test]\n",
    "finalfile_index=test.id #Index des données de test pour le résultat final\n",
    "\n",
    "# Useless column\n",
    "train.drop('num_private', axis=1, inplace=True)\n",
    "train.drop('date_recorded', axis=1, inplace=True)\n",
    "train.drop('funder', axis=1, inplace=True)\n",
    "train.drop('installer', axis=1, inplace=True)\n",
    "train.drop('scheme_name', axis=1, inplace=True)\n",
    "train.drop('scheme_management', axis=1, inplace=True)\n",
    "train.drop('recorded_by', axis=1, inplace=True)\n",
    "train.drop('wpt_name', axis=1, inplace=True)\n",
    "train.drop('subvillage', axis=1, inplace=True)\n",
    "test.drop('num_private', axis=1, inplace=True)\n",
    "test.drop('date_recorded', axis=1, inplace=True)\n",
    "test.drop('funder', axis=1, inplace=True)\n",
    "test.drop('installer', axis=1, inplace=True)\n",
    "test.drop('scheme_name', axis=1, inplace=True)\n",
    "test.drop('scheme_management', axis=1, inplace=True)\n",
    "test.drop('recorded_by', axis=1, inplace=True)\n",
    "test.drop('subvillage', axis=1, inplace=True)\n",
    "test.drop('wpt_name', axis=1, inplace=True)\n",
    "\n",
    "# Double column\n",
    "train.drop('quantity', axis=1, inplace=True)\n",
    "train.drop('source', axis=1, inplace=True)\n",
    "train.drop('waterpoint_type', axis=1, inplace=True)\n",
    "train.drop('payment', axis=1, inplace=True)\n",
    "train.drop('management', axis=1, inplace=True)\n",
    "train.drop('extraction_type', axis=1, inplace=True)\n",
    "train.drop('extraction_type_group', axis=1, inplace=True)\n",
    "train.drop('water_quality', axis=1, inplace=True)\n",
    "train.drop('source_type', axis=1, inplace=True)\n",
    "train.drop('ward', axis=1, inplace=True)\n",
    "train.drop('lga', axis=1, inplace=True)\n",
    "train.drop('region', axis=1, inplace=True)\n",
    "train.drop('basin', axis=1, inplace=True)\n",
    "train.drop('longitude', axis=1, inplace=True)\n",
    "train.drop('latitude', axis=1, inplace=True)\n",
    "test.drop('quantity', axis=1, inplace=True)\n",
    "test.drop('source', axis=1, inplace=True)\n",
    "test.drop('waterpoint_type', axis=1, inplace=True)\n",
    "test.drop('payment', axis=1, inplace=True)\n",
    "test.drop('management', axis=1, inplace=True)\n",
    "test.drop('extraction_type', axis=1, inplace=True)\n",
    "test.drop('extraction_type_group', axis=1, inplace=True)\n",
    "test.drop('water_quality', axis=1, inplace=True)\n",
    "test.drop('source_type', axis=1, inplace=True)\n",
    "test.drop('ward', axis=1, inplace=True)\n",
    "test.drop('lga', axis=1, inplace=True)\n",
    "test.drop('region', axis=1, inplace=True)\n",
    "test.drop('basin', axis=1, inplace=True)\n",
    "test.drop('longitude', axis=1, inplace=True)\n",
    "test.drop('latitude', axis=1, inplace=True)\n",
    "\n",
    "# Refactoring value empty\n",
    "for dataset in full_data:\n",
    "    dataset.loc[dataset.public_meeting.isnull(), 'public_meeting'] = 0\n",
    "    dataset.loc[dataset.public_meeting == False, 'public_meeting'] = 0\n",
    "    dataset.loc[dataset.public_meeting == True, 'public_meeting'] = 1\n",
    "    # dataset.loc[dataset.latitude == 0.0, 'latitude'] = 35.225766\n",
    "    # dataset.loc[dataset.longitude == '-2,00E-08', 'longitude'] = -6.145943\n",
    "    dataset.loc[dataset.extraction_type_class == 'gravity', 'extraction_type_class'] = 0\n",
    "    dataset.loc[dataset.extraction_type_class == 'submersible', 'extraction_type_class'] = 1\n",
    "    dataset.loc[dataset.extraction_type_class == 'handpump', 'extraction_type_class'] = 2\n",
    "    dataset.loc[dataset.extraction_type_class == 'other', 'extraction_type_class'] = 3\n",
    "    dataset.loc[dataset.extraction_type_class == 'motorpump', 'extraction_type_class'] = 4\n",
    "    dataset.loc[dataset.extraction_type_class == 'wind-powered', 'extraction_type_class'] = 5\n",
    "    dataset.loc[dataset.extraction_type_class == 'rope pump', 'extraction_type_class'] = 6\n",
    "    dataset.loc[dataset.management_group == 'user-group', 'management_group'] = 0\n",
    "    dataset.loc[dataset.management_group == 'unknown', 'management_group'] = 1\n",
    "    dataset.loc[dataset.management_group == 'other', 'management_group'] = 1\n",
    "    dataset.loc[dataset.management_group == 'commercial', 'management_group'] = 2\n",
    "    dataset.loc[dataset.management_group == 'parastatal', 'management_group'] = 3\n",
    "    dataset.loc[dataset.payment_type == 'annually', 'payment_type'] = 0\n",
    "    dataset.loc[dataset.payment_type == 'never pay', 'payment_type'] = 1\n",
    "    dataset.loc[dataset.payment_type == 'per bucket', 'payment_type'] = 2\n",
    "    dataset.loc[dataset.payment_type == 'unknown', 'payment_type'] = 3\n",
    "    dataset.loc[dataset.payment_type == 'other', 'payment_type'] = 3\n",
    "    dataset.loc[dataset.payment_type == 'on failure', 'payment_type'] = 4\n",
    "    dataset.loc[dataset.payment_type == 'never pay', 'payment_type'] = 5\n",
    "    dataset.loc[dataset.payment_type == 'monthly', 'payment_type'] = 6\n",
    "    dataset.loc[dataset.quality_group == 'good', 'quality_group'] = 0\n",
    "    dataset.loc[dataset.quality_group == 'salty', 'quality_group'] = 1\n",
    "    dataset.loc[dataset.quality_group == 'milky', 'quality_group'] = 2\n",
    "    dataset.loc[dataset.quality_group == 'fluoride', 'quality_group'] = 3\n",
    "    dataset.loc[dataset.quality_group == 'colored', 'quality_group'] = 4\n",
    "    dataset.loc[dataset.quality_group == 'unknown', 'quality_group'] = 5\n",
    "    dataset.loc[dataset.quantity_group == 'enough', 'quantity_group'] = 0\n",
    "    dataset.loc[dataset.quantity_group == 'insufficient', 'quantity_group'] = 1\n",
    "    dataset.loc[dataset.quantity_group == 'dry', 'quantity_group'] = 2\n",
    "    dataset.loc[dataset.quantity_group == 'seasonal', 'quantity_group'] = 3\n",
    "    dataset.loc[dataset.quantity_group == 'unknown', 'quantity_group'] = 4\n",
    "    dataset.loc[dataset.source_class == 'groundwater', 'source_class'] = 0\n",
    "    dataset.loc[dataset.source_class == 'surface', 'source_class'] = 1\n",
    "    dataset.loc[dataset.source_class == 'unknown', 'source_class'] = 2\n",
    "    dataset.loc[dataset.waterpoint_type_group == 'communal standpipe', 'waterpoint_type_group'] = 0\n",
    "    dataset.loc[dataset.waterpoint_type_group == 'hand pump', 'waterpoint_type_group'] = 1\n",
    "    dataset.loc[dataset.waterpoint_type_group == 'other', 'waterpoint_type_group'] = 2\n",
    "    dataset.loc[dataset.waterpoint_type_group == 'improved spring', 'waterpoint_type_group'] = 3\n",
    "    dataset.loc[dataset.waterpoint_type_group == 'cattle trough', 'waterpoint_type_group'] = 4\n",
    "    dataset.loc[dataset.waterpoint_type_group == 'dam', 'waterpoint_type_group'] = 5\n",
    "    dataset.loc[dataset.permit.isnull(), 'permit'] = 0\n",
    "    dataset.loc[dataset.permit == False, 'permit'] = 0\n",
    "    dataset.loc[dataset.permit == True, 'permit'] = 1\n",
    "    dataset.loc[dataset.construction_year == 0, 'construction_year'] = np.NaN\n",
    "    construction_year_avg = dataset['construction_year'].mean() # Calcul de la valeur moyenne\n",
    "    construction_year_std = dataset['construction_year'].std()  # Calcul de l'écart type\n",
    "    construction_year_null_count = dataset['construction_year'].isnull().sum() # nombre de valeur nulle\n",
    "    construction_year_null_random_list = np.random.randint(construction_year_avg - construction_year_std, construction_year_avg + construction_year_std, size=construction_year_null_count)   \n",
    "    dataset.loc[np.isnan(dataset['construction_year']),'construction_year'] = construction_year_null_random_list    \n",
    "    dataset['construction_year'] = dataset['construction_year'].astype(int)\n",
    "    \n",
    "    dataset.loc[dataset.population == 0, 'population'] = np.NaN\n",
    "    population_avg = dataset['population'].mean() # Calcul de la valeur moyenne\n",
    "    population_std = dataset['population'].std()  # Calcul de l'écart type\n",
    "    population_null_count = dataset['population'].isnull().sum() # nombre de valeur nulle\n",
    "    population_null_random_list = np.random.randint(population_avg, population_avg + population_std, size=population_null_count)   \n",
    "    dataset.loc[np.isnan(dataset['population']),'population'] = population_null_random_list    \n",
    "    dataset['population'] = dataset['population'].astype(int)\n",
    "    # dataset.loc[dataset.management_group == 'unknown', 'management_group'] = 'other'\n",
    "    # dataset.loc[dataset.payment_type == 'unknown', 'payment_type'] = 'other'\n",
    "\n",
    "\n",
    "train['functional']= (train['status_group']=='functional')*1\n",
    "train['non_functional']= (train['status_group']=='non functional')*1\n",
    "train['functional_needs_repair']= (train['status_group']=='functional needs repair')*1\n",
    "\n",
    "train.drop('status_group', axis=1, inplace=True)\n",
    "\n",
    "BASE_DIR = os.path.abspath('')\n",
    "# print(BASE_DIR)\n",
    "train.to_csv(os.path.join(BASE_DIR, 'version_final.csv'))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "#Séparation des valeurs de train et label (tous les exemples)\n",
    "X_alltrain = train.values[:, 1:]\n",
    "y_alltrain = train.values[:, 0]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_alltrain, y_alltrain, random_state=42)\n",
    "print('%i X_train, %i X_test, %i y_train, %i y_test'%(\n",
    "    X_train.shape[0], X_test.shape[0], y_train.shape[0], y_test.shape[0]))\n",
    "print('%i X_alltrain, %i y_alltrain'%(X_alltrain.shape[0], y_alltrain.shape[0]))\n",
    "feature_names=train.columns.tolist()[1:-3]\n",
    "target_names=[\"functional\",\"non_functional\",\"functional needs repair\"]\n",
    "print('features:',feature_names)\n",
    "print('target:',target_names)\n",
    "\n",
    "# Algorithm à utiliser : XGBClassifier to Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1, random_state=42)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "for name, score in zip(feature_names, rnd_clf.feature_importances_):\n",
    "    print('%s: %i%%' %(name, int(score*100)))\n",
    "    \n",
    "\n",
    "rnd_clf_short = RandomForestClassifier(n_estimators=500, n_jobs=-1, random_state=42)\n",
    "rnd_clf_short.fit(train[['quality_group','quantity_group','region_code','construction_year']].values, y_alltrain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
