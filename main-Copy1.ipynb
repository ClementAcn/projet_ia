{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traning data: successfully\n",
      "Features for trainig: successfully\n",
      "Assign data for validation: successfully\n",
      "Classifier: successfully\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'gravity'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-c4b07bac7816>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;31m# Traning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumn_labels\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"status_group\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Traning: successfully\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m         \u001b[1;31m# Validate or convert input data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 250\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"csc\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    525\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    526\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'error'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 527\u001b[1;33m                 \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    528\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    529\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m     \"\"\"\n\u001b[1;32m--> 538\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'gravity'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re as re\n",
    "import os.path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "# from tensorflow.keras.callbacks import History,LearningRateScheduler\n",
    "# from tensorflow.keras.layers import Dropout\n",
    "# print('TensorFlow %s, Keras %s, numpy %s, pandas %s'%(tf.__version__,keras.__version__, np.__version__,pd.__version__))\n",
    "__DEBUG__=False\n",
    "\n",
    "train = pd.read_csv('data_concatener.csv', sep=';')\n",
    "train = train.iloc[np.random.permutation(len(train))]\n",
    "test  = pd.read_csv('test_set_values.csv')\n",
    "full_data = [train, test]\n",
    "finalfile_index=test.id #Index des données de test pour le résultat final\n",
    "\n",
    "# Useless column\n",
    "train.drop('num_private', axis=1, inplace=True)\n",
    "train.drop('date_recorded', axis=1, inplace=True)\n",
    "train.drop('funder', axis=1, inplace=True)\n",
    "train.drop('installer', axis=1, inplace=True)\n",
    "train.drop('scheme_name', axis=1, inplace=True)\n",
    "train.drop('scheme_management', axis=1, inplace=True)\n",
    "train.drop('recorded_by', axis=1, inplace=True)\n",
    "train.drop('wpt_name', axis=1, inplace=True)\n",
    "train.drop('subvillage', axis=1, inplace=True)\n",
    "test.drop('num_private', axis=1, inplace=True)\n",
    "test.drop('date_recorded', axis=1, inplace=True)\n",
    "test.drop('funder', axis=1, inplace=True)\n",
    "test.drop('installer', axis=1, inplace=True)\n",
    "test.drop('scheme_name', axis=1, inplace=True)\n",
    "test.drop('scheme_management', axis=1, inplace=True)\n",
    "test.drop('recorded_by', axis=1, inplace=True)\n",
    "test.drop('subvillage', axis=1, inplace=True)\n",
    "test.drop('wpt_name', axis=1, inplace=True)\n",
    "\n",
    "# Double column\n",
    "train.drop('quantity', axis=1, inplace=True)\n",
    "train.drop('source', axis=1, inplace=True)\n",
    "train.drop('waterpoint_type', axis=1, inplace=True)\n",
    "train.drop('payment', axis=1, inplace=True)\n",
    "train.drop('management', axis=1, inplace=True)\n",
    "train.drop('extraction_type', axis=1, inplace=True)\n",
    "train.drop('extraction_type_group', axis=1, inplace=True)\n",
    "train.drop('water_quality', axis=1, inplace=True)\n",
    "train.drop('source_type', axis=1, inplace=True)\n",
    "train.drop('ward', axis=1, inplace=True)\n",
    "train.drop('lga', axis=1, inplace=True)\n",
    "train.drop('region', axis=1, inplace=True)\n",
    "train.drop('basin', axis=1, inplace=True)\n",
    "train.drop('longitude', axis=1, inplace=True)\n",
    "train.drop('latitude', axis=1, inplace=True)\n",
    "test.drop('quantity', axis=1, inplace=True)\n",
    "test.drop('source', axis=1, inplace=True)\n",
    "test.drop('waterpoint_type', axis=1, inplace=True)\n",
    "test.drop('payment', axis=1, inplace=True)\n",
    "test.drop('management', axis=1, inplace=True)\n",
    "test.drop('extraction_type', axis=1, inplace=True)\n",
    "test.drop('extraction_type_group', axis=1, inplace=True)\n",
    "test.drop('water_quality', axis=1, inplace=True)\n",
    "test.drop('source_type', axis=1, inplace=True)\n",
    "test.drop('ward', axis=1, inplace=True)\n",
    "test.drop('lga', axis=1, inplace=True)\n",
    "test.drop('region', axis=1, inplace=True)\n",
    "test.drop('basin', axis=1, inplace=True)\n",
    "test.drop('longitude', axis=1, inplace=True)\n",
    "test.drop('latitude', axis=1, inplace=True)\n",
    "\n",
    "# Refactoring value empty\n",
    "for dataset in full_data:\n",
    "    dataset.loc[dataset.public_meeting.isnull(), 'public_meeting'] = 0\n",
    "    dataset.loc[dataset.public_meeting == False, 'public_meeting'] = 0\n",
    "    dataset.loc[dataset.public_meeting == True, 'public_meeting'] = 1\n",
    "    # dataset.loc[dataset.latitude == 0.0, 'latitude'] = 35.225766\n",
    "    # dataset.loc[dataset.longitude == '-2,00E-08', 'longitude'] = -6.145943\n",
    "    dataset.loc[dataset.extraction_type_class == 'gravity', 'extraction_type_class'] = 0\n",
    "    dataset.loc[dataset.extraction_type_class == 'submersible', 'extraction_type_class'] = 1\n",
    "    dataset.loc[dataset.extraction_type_class == 'handpump', 'extraction_type_class'] = 2\n",
    "    dataset.loc[dataset.extraction_type_class == 'other', 'extraction_type_class'] = 3\n",
    "    dataset.loc[dataset.extraction_type_class == 'motorpump', 'extraction_type_class'] = 4\n",
    "    dataset.loc[dataset.extraction_type_class == 'wind-powered', 'extraction_type_class'] = 5\n",
    "    dataset.loc[dataset.extraction_type_class == 'rope pump', 'extraction_type_class'] = 6\n",
    "    dataset.loc[dataset.management_group == 'user-group', 'management_group'] = 0\n",
    "    dataset.loc[dataset.management_group == 'unknown', 'management_group'] = 1\n",
    "    dataset.loc[dataset.management_group == 'other', 'management_group'] = 1\n",
    "    dataset.loc[dataset.management_group == 'commercial', 'management_group'] = 2\n",
    "    dataset.loc[dataset.management_group == 'parastatal', 'management_group'] = 3\n",
    "    dataset.loc[dataset.payment_type == 'annually', 'payment_type'] = 0\n",
    "    dataset.loc[dataset.payment_type == 'never pay', 'payment_type'] = 1\n",
    "    dataset.loc[dataset.payment_type == 'per bucket', 'payment_type'] = 2\n",
    "    dataset.loc[dataset.payment_type == 'unknown', 'payment_type'] = 3\n",
    "    dataset.loc[dataset.payment_type == 'other', 'payment_type'] = 3\n",
    "    dataset.loc[dataset.payment_type == 'on failure', 'payment_type'] = 4\n",
    "    dataset.loc[dataset.payment_type == 'never pay', 'payment_type'] = 5\n",
    "    dataset.loc[dataset.payment_type == 'monthly', 'payment_type'] = 6\n",
    "    dataset.loc[dataset.quality_group == 'good', 'quality_group'] = 0\n",
    "    dataset.loc[dataset.quality_group == 'salty', 'quality_group'] = 1\n",
    "    dataset.loc[dataset.quality_group == 'milky', 'quality_group'] = 2\n",
    "    dataset.loc[dataset.quality_group == 'fluoride', 'quality_group'] = 3\n",
    "    dataset.loc[dataset.quality_group == 'colored', 'quality_group'] = 4\n",
    "    dataset.loc[dataset.quality_group == 'unknown', 'quality_group'] = 5\n",
    "    dataset.loc[dataset.quantity_group == 'enough', 'quantity_group'] = 0\n",
    "    dataset.loc[dataset.quantity_group == 'insufficient', 'quantity_group'] = 1\n",
    "    dataset.loc[dataset.quantity_group == 'dry', 'quantity_group'] = 2\n",
    "    dataset.loc[dataset.quantity_group == 'seasonal', 'quantity_group'] = 3\n",
    "    dataset.loc[dataset.quantity_group == 'unknown', 'quantity_group'] = 4\n",
    "    dataset.loc[dataset.source_class == 'groundwater', 'source_class'] = 0\n",
    "    dataset.loc[dataset.source_class == 'surface', 'source_class'] = 1\n",
    "    dataset.loc[dataset.source_class == 'unknown', 'source_class'] = 2\n",
    "    dataset.loc[dataset.waterpoint_type_group == 'communal standpipe', 'waterpoint_type_group'] = 0\n",
    "    dataset.loc[dataset.waterpoint_type_group == 'hand pump', 'waterpoint_type_group'] = 1\n",
    "    dataset.loc[dataset.waterpoint_type_group == 'other', 'waterpoint_type_group'] = 2\n",
    "    dataset.loc[dataset.waterpoint_type_group == 'improved spring', 'waterpoint_type_group'] = 3\n",
    "    dataset.loc[dataset.waterpoint_type_group == 'cattle trough', 'waterpoint_type_group'] = 4\n",
    "    dataset.loc[dataset.waterpoint_type_group == 'dam', 'waterpoint_type_group'] = 5\n",
    "    dataset.loc[dataset.permit.isnull(), 'permit'] = 0\n",
    "    dataset.loc[dataset.permit == False, 'permit'] = 0\n",
    "    dataset.loc[dataset.permit == True, 'permit'] = 1\n",
    "    dataset.loc[dataset.construction_year == 0, 'construction_year'] = np.NaN\n",
    "    construction_year_avg = dataset['construction_year'].mean() # Calcul de la valeur moyenne\n",
    "    construction_year_std = dataset['construction_year'].std()  # Calcul de l'écart type\n",
    "    construction_year_null_count = dataset['construction_year'].isnull().sum() # nombre de valeur nulle\n",
    "    construction_year_null_random_list = np.random.randint(construction_year_avg - construction_year_std, construction_year_avg + construction_year_std, size=construction_year_null_count)   \n",
    "    dataset.loc[np.isnan(dataset['construction_year']),'construction_year'] = construction_year_null_random_list    \n",
    "    dataset['construction_year'] = dataset['construction_year'].astype(int)\n",
    "    \n",
    "    dataset.loc[dataset.population == 0, 'population'] = np.NaN\n",
    "    population_avg = dataset['population'].mean() # Calcul de la valeur moyenne\n",
    "    population_std = dataset['population'].std()  # Calcul de l'écart type\n",
    "    population_null_count = dataset['population'].isnull().sum() # nombre de valeur nulle\n",
    "    population_null_random_list = np.random.randint(population_avg, population_avg + population_std, size=population_null_count)   \n",
    "    dataset.loc[np.isnan(dataset['population']),'population'] = population_null_random_list    \n",
    "    dataset['population'] = dataset['population'].astype(int)\n",
    "    # dataset.loc[dataset.management_group == 'unknown', 'management_group'] = 'other'\n",
    "    # dataset.loc[dataset.payment_type == 'unknown', 'payment_type'] = 'other'\n",
    "\n",
    "train['status_group'] = train['status_group'].map({'functional': int(2), 'functional needs repair': int(1), 'non functional': int(0)})\n",
    "BASE_DIR = os.path.abspath('')\n",
    "# print(BASE_DIR)\n",
    "train.to_csv(os.path.join(BASE_DIR, 'version_final.csv'))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "#Séparation des valeurs de train et label (tous les exemples)\n",
    "X_alltrain = train.values[:, 1:]\n",
    "y_alltrain = train['status_group']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_alltrain, y_alltrain, random_state=10)\n",
    "print('%i X_train, %i X_test, %i y_train, %i y_test'%(\n",
    "    X_train.shape[0], X_test.shape[0], y_train.shape[0], y_test.shape[0]))\n",
    "# print('%i X_alltrain, %i y_alltrain'%(X_alltrain.shape[0], y_alltrain.shape[0]))\n",
    "feature_names=train.columns.tolist()[1:-3]\n",
    "target_names=['status_group']\n",
    "print('features:',feature_names)\n",
    "print('target:',target_names)\n",
    "\n",
    "# Algorithm à utiliser : XGBClassifier to Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_jobs=-1, n_estimators=700)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "# for name, score in zip(feature_names, rnd_clf.feature_importances_):\n",
    "#     print('%s: %i%%' %(name, int(score*100)))\n",
    "\n",
    "print('____________________________')\n",
    "accuracy = accuracy_score(rnd_clf.predict(X_alltrain), y_alltrain)\n",
    "print(\"Accuracy = \" + str(accuracy))\n",
    "print(\"Accuracy: successfully\")\n",
    "print('____________________________')\n",
    "# rnd_clf_short = RandomForestClassifier(n_jobs=-1, n_estimators=200)\n",
    "# rnd_clf_short.fit(train[['quality_group','quantity_group','region_code','construction_year']].values, y_alltrain)\n",
    "# for name, score in zip(feature_names, rnd_clf_short.feature_importances_):\n",
    "#     print('%s: %i%%' %(name, int(score*100)))\n",
    "accuracy = accuracy_score(rnd_clf.predict(X_alltrain), y_alltrain)\n",
    "print(\"Accuracy = \" + str(accuracy))\n",
    "print(\"Accuracy: successfully\")\n",
    "print('____________________________')\n",
    "prediction = rnd_clf.predict(test)\n",
    "status_group = [\"functional\", \"non functional\", \"functional needs repair\"]\n",
    "print(\"Prediction: successfully\")\n",
    "print('____________________________')\n",
    "submission = pd.DataFrame({\n",
    "\t\t\t\"id\": test[\"id\"],\n",
    "\t\t\t\"status_group\": prediction\n",
    "\t\t})\n",
    "for i in range(len(status_group)):\n",
    "\tsubmission.loc[submission[\"status_group\"] == i, \"status_group\"] = status_group[i]\n",
    "\n",
    "submission.to_csv(\"submission.csv\", index = False)\n",
    "print(\"Store submission : successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
